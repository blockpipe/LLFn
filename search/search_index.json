{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to LLFn","text":"<p>\ud83d\udc4b Welcome to LLFn documentation page.</p>"},{"location":"advanced_features/","title":"Advanced Features","text":"<p>While LLFn is simple and light-weight by design, it does pack some punches above its weight that helps you write AI applications more intuitively.</p>"},{"location":"advanced_features/#structured-function-output","title":"\ud83d\udcd0 Structured Function Output","text":"<p>You can create a function to automatically format the response into a desired <code>pydantic</code> object. To do this, you simply declare the output of your function in <code>pydantic</code>, and let the LLFn do the heavy lifting for you.</p> <pre><code>from pydantic import BaseModel, Field\n\nclass AgentLogicOutput(BaseModel):\n  tasks: List[str] = Field(..., description=\"list of tasks to accomplish the given goal\")\n  reasoning: str  = Field(..., description=\"why you choose to do these tasks\")\n\n@function_prompt\ndef agent_think(goal: str) -&gt; AgentLogicOutput:\n    return f\"You're an agent planning 5 tasks to accomplish the following goal: {goal}\"\n\nagent_think(\"Creating online business\") # this returns AgentLogicOutput object\n</code></pre> <p>This allows for powerful composability like tools, agents, etc. without sacrificing the simplicity and verbosity of our program. Under the hood LLFn injects the output format into your prompt and automatically parse the LLM result into the specified format.</p>"},{"location":"advanced_features/#binding-and-overriding-llms","title":"\ud83d\udddd\ufe0f Binding and Overriding LLMs","text":"<p>LLFn let you use both LangChain's LLMs and Chat Models to power your functions. There are 2 ways to assign it.</p>"},{"location":"advanced_features/#method-1-binding-llm-to-all-your-functions","title":"Method 1: Binding LLM to All Your Functions","text":"<p>This is the most convenient way to just get your function to work.</p> <pre><code>from llfn import LLFn\nfrom langchain.chat_models import ChatOpenAI\n\nllm = ChatOpenAI(temperature=0.7, openai_api_key=API_KEY)\n\nfunction_prompt = LLFn()\nfunction_prompt.bind(llm)\n\n@function_prompt\ndef your_function(...):\n    ...\n</code></pre>"},{"location":"advanced_features/#method-2-bindingoverriding-llm-to-a-specific-function","title":"Method 2: Binding/Overriding LLM to A Specific Function","text":"<p>If you want to use different LLMs for different tasks, you can do so by binding <code>llm</code> to your function individually.</p> <pre><code>from langchain.llms import LlamaCpp\n\nllm2 = LlamaCpp(...)\n\nyour_function.bind(llm2) # This will override the LLM for this function\n</code></pre>"},{"location":"advanced_features/#callbacks-and-streaming","title":"\ud83c\udfc4\u200d\u2642\ufe0f Callbacks and Streaming","text":""},{"location":"advanced_features/#callbacks","title":"Callbacks","text":"<p>With LLFn, it's trivial to define callbacks in between execution. Because everything is a function, you can <code>print</code> or call any 3rd party APIs anywhere you want.</p> <p>For example, if you want to print results of executions in between a series of complex AI agent it would look like this.</p> <pre><code>...\n\n# Assuming that other functions are already defined with function_prompt\n@function_prompt\ndef agent_plan_and_execute(goal: str) -&gt; str:\n    tasks = agent_think(goal)\n    print(tasks) # Debug\n    results = agent_execute_tasks(tasks)\n    print(results) # Debug\n    evaluation = agent_evaluate_perf(goal, results)\n    print(evaluation) # Debug\n</code></pre> <p>Your function can also take a callback function as a parameter. LLFn does not dictate how you should do it. Anything that works for you would do!</p>"},{"location":"advanced_features/#streaming-llm-output","title":"Streaming LLM Output","text":"<p>Because LLFn leverages LangChain's LLMs, you can use LangChain Streaming directly.</p> <pre><code>from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n\nllm = ChatOpenAI(\n    streaming=True,\n    callbacks=[StreamingStdOutCallbackHandler()],\n    ...\n)\n\nfunction_prompt.bind(llm)\n</code></pre>"},{"location":"core_concept/","title":"Core Concept","text":""},{"location":"core_concept/#defining-your-function","title":"\u2b50\ufe0f Defining Your Function","text":"<p>The primary goal of LLFn is to encapsulate everything you do with LLMs into a function. Each function comprises of 3 components: <code>input</code>, <code>prompt</code>, and <code>output</code>. Here's how you can create a simple text translator using LLFn.</p> <pre><code>from llfn import LLFn\n\nfunction_prompt = LLFn()\n\n@function_prompt\ndef translate(text: str, output_language: str) -&gt; str:\n    return f\"Translate the following text into {output_language} language: {text}\"\n</code></pre>"},{"location":"core_concept/#binding-it-with-llm","title":"\ud83e\udd6a Binding It with LLM","text":"<p>To execute this you have to <code>bind</code> the function to an LLM. You can bind any LangChain's supported language models to LLFn functions.</p> <pre><code>from langchain.chat_models import ChatOpenAI\n\n# Setup LLM and bind it to the function\nllm = ChatOpenAI(temperature=0.7, openai_api_key=API_KEY)\ntranslate.bind(llm)\n</code></pre> <p>If you don't want to repeat yourself binding all the functions individually, you can bind directly to <code>function_prompt</code> as well.</p> <pre><code># Alternatively: bind the llm to all functions\nfunction_prompt.bind(llm)\n</code></pre>"},{"location":"core_concept/#calling-the-function","title":"\ud83c\udf7a Calling the Function","text":"<p>Now you can call your function like you would for any Python function.</p> <pre><code>translate(\"Hello welcome. How are you?\", \"Thai\")\n</code></pre> <p>The beauty of this construct is that you're able to infinitely compose your applications. LLFn does not make any assumption on how you'd chain up your application logic. The only requirement for a <code>function_prompt</code> is that it returns a prompt string at the end.</p> <p>And ... that's it! That's just about everything you need to learn about LLFn to start building AI apps with it.</p> \ud83d\udc46 Click to see the full code <p> <pre><code>import os\nfrom llfn import LLFn\nfrom langchain.chat_models import ChatOpenAI\n\nllm = ChatOpenAI(\n    temperature=0.7,\n    model=os.getenv(\"OPENAI_MODEL\"),\n    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n)\n\nfunction_prompt = LLFn()\nfunction_prompt.bind(llm)\n\n@function_prompt\ndef translate(text: str, output_language: str) -&gt; str:\n    return f\"Translate the following text into {output_language} language: {text}\"\n\n# Call the function\nprint(translate(\"Hello welcome. How are you?\", \"Thai\"))\n</code></pre> </p>"},{"location":"features/","title":"Features","text":"<p>LLFn has only one goal: making it 100x easier to experiment and ship AI applications using the LLMs with minimum boilerplate. Even if you're new to LLMs, you can still get started and be productive in minutes, instead of spending hours learning how to use the framework.</p> <ul> <li>\ud83d\udd0b Functionify: turning any <code>prompt</code> into a callable function.</li> <li>\ud83d\udce4 Flexible Output: defining LLM result with Python and <code>pydantic</code> types.</li> <li>\ud83e\uddf1 Infinitely Composable: any function can call any other function in any order.</li> <li>\ud83d\uded2 Use Any LLM: leveraging <code>LangChain</code>'s LLM interface, drop in your favorite LLM with 100% compatibility.</li> <li>\ud83e\udeb6 Light-Weight: small core framework of LLFn making it extremely easy to understand and extend.</li> </ul> <p>We draw a lot of inspiration from both FastAPI and LangChain, so if you're familiar with either of them you'd feel right at home with LLFn.</p>"},{"location":"installation/","title":"Installation","text":"<p>LLFn is available on PyPi. You can install LLFn using your favorite Python package management software.</p> <pre><code>$ pip install llfn # If you use pip\n$ poetry add llfn # If you use poetry\n</code></pre>"}]}